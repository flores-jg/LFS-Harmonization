{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1701a92",
   "metadata": {},
   "source": [
    "# LFS Harmonizer v8 Notebook\n",
    "Refactors the LFS harmonizer script into reusable notebook cells for single-file and batch processing.\n",
    "\n",
    "Dependencies:\n",
    "- pandas\n",
    "- pyarrow (for parquet)\n",
    "\n",
    "If needed:\n",
    "\n",
    "```\n",
    "pip install pandas pyarrow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9a5c7",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Paths\n",
    "Set input/output paths, batch size, and runtime options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947ca304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "input_dir = Path(\"./raw\")\n",
    "output_dir = Path(\"./output_v8\")\n",
    "\n",
    "# batch processing \n",
    "batch_size = 5\n",
    "\n",
    "single_file = \"\"\n",
    "\n",
    "# Basic runtime options\n",
    "show_samples = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976536ee",
   "metadata": {},
   "source": [
    "## 2. Define Column Priority and Output Schema\n",
    "Standardize field selection and ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b172ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_PRIORITY = {\n",
    "    # Survey identifiers\n",
    "    \"PUFREG\": [\"PUFREG\", \"CREG\", \"REG\"],\n",
    "    \"PUFSVYYR\": [\"PUFSVYYR\", \"SVYYR\", \"CYEAR\"],\n",
    "    \"PUFSVYMO\": [\"PUFSVYMO\", \"SVYMO\", \"CMONTH\"],\n",
    "    \"PUFHHNUM\": [\"PUFHHNUM\", \"HHNUM\"],\n",
    "    \"PUFPSU\": [\"PUFPSU\", \"PSU\", \"PSU_NO\", \"STRATUM\"],\n",
    "    \"PUFHHSIZE\": [\"PUFHHSIZE\", \"HHID\"],\n",
    "    \"PUFRPL\": [\"PUFRPL\", \"CRPM\"],\n",
    "\n",
    "    # Weight\n",
    "    \"PUFPWGTPRV\": [\"PUFPWGTPRV\", \"PUFPWGT\", \"PUFPWGTFIN\", \"CFWGT\", \"FWGT\", \"PWGT\"],\n",
    "\n",
    "    # Demographics\n",
    "    \"PUFC01_LNO\": [\"PUFC01_LNO\", \"C101_LNO\", \"CC101_LNO\", \"C04_LNO\", \"A01_LNO\"],\n",
    "    \"PUFC03_REL\": [\"PUFC03_REL\", \"C05_REL\", \"CC05_REL\", \"C03_NEWMEM\", \"CC03_NEWMEM\"],\n",
    "    \"PUFC04_SEX\": [\"PUFC04_SEX\", \"C06_SEX\", \"CC06_SEX\"],\n",
    "    \"PUFC05_AGE\": [\"PUFC05_AGE\", \"C07_AGE\", \"CC07_AGE\"],\n",
    "    \"PUFC06_MSTAT\": [\"PUFC06_MSTAT\", \"C08_MSTAT\", \"C08_MS\", \"CC08_MSTAT\", \"CC08_MS\"],\n",
    "    \"PUFC07_GRADE\": [\"PUFC07_GRADE\", \"J12C09_GRADE\", \"C09_GRD\", \"C09_GRADE\", \"CC09_GRADE\"],\n",
    "    \"PUFC08_CURSCH\": [\"PUFC08_CURSCH\", \"A02_CURSCH\", \"A02_CSCH\"],\n",
    "    \"PUFC09_GRADTECH\": [\"PUFC09_GRADTECH\", \"J12C11_GRADTECH\", \"J12C11COURSE\"],\n",
    "\n",
    "    # Employment status\n",
    "    \"PUFC10_CONWR\": [\"PUFC10_CONWR\", \"PUFC08_CONWR\", \"C10_CONWR\", \"C10_CNWR\", \"CC10_CONWR\"],\n",
    "    \"PUFC11_WORK\": [\"PUFC11_WORK\", \"PUFC09_WORK\", \"C13_WORK\", \"CC13_WORK\", \"CC01_WORK\", \"B01_WORK\"],\n",
    "    \"PUFC12_JOB\": [\"PUFC12_JOB\", \"PUFC10_JOB\", \"C14_JOB\", \"CC14_JOB\", \"CC02_JOB\", \"B02_JOB\"],\n",
    "    \"PUFNEWEMPSTAT\": [\"PUFNEWEMPSTAT\", \"NEWEMPSTAT\", \"CEMPST1\", \"CEMPST2\", \"NEWEMPST\"],\n",
    "\n",
    "    # Occupation and industry\n",
    "    \"PUFC14_PROCC\": [\"PUFC14_PROCC\", \"PUFC13_PROCC\", \"C16_PROCC\", \"C16_PROC\", \"CC16_PROCC\",\n",
    "                      \"C16F2_PROCC\", \"C16L2_PROCC\", \"CC12_USOCC\", \"J01_USOCC\", \"J01_USOC\"],\n",
    "    \"PUFC16_PKB\": [\"PUFC16_PKB\", \"PUFC15_PKB\", \"C18_PKB\", \"CC18_PKB\",\n",
    "                    \"C18F2_PKB\", \"C18L2_PKB\", \"CC06_IND\", \"J03_OKB\"],\n",
    "    \"PUFC17_NATEM\": [\"PUFC17_NATEM\", \"PUFC16_NATEM\", \"C20_NATEM\", \"C20_NTEM\", \"CC20_NATEM\"],\n",
    "\n",
    "    # Working hours\n",
    "    \"PUFC18_PNWHRS\": [\"PUFC18_PNWHRS\", \"PUFC17_PNWHRS\", \"C21_PNWHRS\", \"C21_PWHR\", \"CC21_PNWHRS\", \"CC18_PNWHRS\"],\n",
    "    \"PUFC19_PHOURS\": [\"PUFC19_PHOURS\", \"PUFC18_PHOURS\", \"C22_PHOURS\", \"C22_PHRS\", \"CC22_PHOURS\"],\n",
    "\n",
    "    # Underemployment\n",
    "    \"PUFC20_PWMORE\": [\"PUFC20_PWMORE\", \"PUFC19_PWMORE\", \"C23_PWMORE\", \"C23_PWMR\", \"CC23_PWMORE\"],\n",
    "    \"PUFC21_PLADDW\": [\"PUFC21_PLADDW\", \"PUFC20_PLADDW\", \"C24_PLADDW\", \"C24_PLAW\", \"CC24_PLADDW\"],\n",
    "    \"PUFC22_PFWRK\": [\"PUFC22_PFWRK\", \"PUFC20B_FTWORK\", \"C25_PFWRK\", \"C25_PFWK\", \"CC25_PFWRK\"],\n",
    "    \"PUFC23_PCLASS\": [\"PUFC23_PCLASS\", \"PUFC21_PCLASS\", \"C19_PCLASS\", \"C19PCLAS\", \"CC19_PCLASS\"],\n",
    "\n",
    "    # Pay\n",
    "    \"PUFC24_PBASIS\": [\"PUFC24_PBASIS\", \"C26_PBASIS\", \"C26_PBIS\", \"CC26_PBASIS\"],\n",
    "    \"PUFC25_PBASIC\": [\"PUFC25_PBASIC\", \"C27_PBASIC\", \"C27_PBSC\", \"CC27_PBASIC\", \"C36_OBASIC\", \"C36_OBIC\"],\n",
    "\n",
    "    # Other job\n",
    "    \"PUFC26_OJOB\": [\"PUFC26_OJOB\", \"PUFC22_OJOB\", \"C28_OJOB\", \"CC28_OJOB\"],\n",
    "\n",
    "    # Multiple jobs\n",
    "    \"PUFC27_NJOBS\": [\"PUFC27_NJOBS\", \"A03_JOBS\"],\n",
    "    \"PUFC28_THOURS\": [\"PUFC28_THOURS\", \"PUFC23_THOURS\", \"A04_THOURS\", \"A04_THRS\"],\n",
    "    \"PUFC29_WWM48H\": [\"PUFC29_WWM48H\", \"PUFC24_WWM48H\", \"A05_RWM48H\", \"A05_R48H\"],\n",
    "\n",
    "    # Job search\n",
    "    \"PUFC30_LOOKW\": [\"PUFC30_LOOKW\", \"PUFC25_LOOKW\", \"C38_LOOKW\", \"C38_LOKW\", \"CC38_LOOKW\", \"CC30_LOOKW\"],\n",
    "    \"PUFC31_FLWRK\": [\"PUFC31_FLWRK\", \"PUFC25B_FTWORK\", \"C41_FLWRK\", \"C41_FLWK\", \"CC41_FLWRK\"],\n",
    "    \"PUFC32_JOBSM\": [\"PUFC32_JOBSM\", \"C39_JOBSM\", \"C39_JBSM\", \"CC39_JOBSM\", \"CC32_JOBSM\"],\n",
    "    \"PUFC33_WEEKS\": [\"PUFC33_WEEKS\", \"C40_WEEKS\", \"C40_WKS\", \"CC40_WEEKS\", \"CC33_WEEKS\"],\n",
    "    \"PUFC34_WYNOT\": [\"PUFC34_WYNOT\", \"PUFC26_WYNOT\", \"C42_WYNOT\", \"C42_WYNT\", \"CC42_WYNOT\"],\n",
    "    \"PUFC35_LTLOOKW\": [\"PUFC35_LTLOOKW\", \"A06_LTLOOKW\", \"A06_LLKW\", \"CC35_LTLOOKW\"],\n",
    "    \"PUFC36_AVAIL\": [\"PUFC36_AVAIL\", \"PUFC27_AVAIL\", \"C37_AVAIL\", \"C37_AVIL\", \"CC37_AVAIL\", \"CC36_AVAIL\"],\n",
    "    \"PUFC37_WILLING\": [\"PUFC37_WILLING\", \"A07_WILLING\", \"A07_WLNG\"],\n",
    "\n",
    "    # Previous work\n",
    "    \"PUFC38_PREVJOB\": [\"PUFC38_PREVJOB\", \"PUFC28_PREVJOB\", \"C43_LBEF\", \"CC43_LBEF\"],\n",
    "    \"PUFC39_YEAR\": [\"PUFC39_YEAR\", \"PUFC29_YEAR\"],\n",
    "    \"PUFC39_MONTH\": [\"PUFC39_MONTH\", \"PUFC29_MONTH\"],\n",
    "    \"PUFC41_POCC\": [\"PUFC41_POCC\", \"PUFC40_POCC\", \"PUFC31_POCC\", \"C45_POCC\", \"CC45_POCC\",\n",
    "                     \"C45F2_POCC\", \"C45L2_POCC\", \"CC10_POCC\"],\n",
    "    \"PUFC43_QKB\": [\"PUFC43_QKB\", \"PUFC33_QKB\", \"A09_PQKB\", \"A09F2_PQKB\", \"A09L2_PQKB\", \"PQKB\", \"QKB\"],\n",
    "}\n",
    "\n",
    "OUTPUT_SCHEMA = [\n",
    "    \"PUFREG\", \"PUFSVYYR\", \"PUFSVYMO\", \"PUFHHNUM\", \"PUFPSU\", \"PUFHHSIZE\", \"PUFRPL\",\n",
    "    \"PUFPWGTPRV\",\n",
    "    \"PUFC01_LNO\", \"PUFC03_REL\", \"PUFC04_SEX\", \"PUFC05_AGE\", \"PUFC06_MSTAT\",\n",
    "    \"PUFC07_GRADE\", \"PUFC08_CURSCH\", \"PUFC09_GRADTECH\",\n",
    "    \"PUFC10_CONWR\", \"PUFC11_WORK\", \"PUFC12_JOB\", \"PUFNEWEMPSTAT\",\n",
    "    \"PUFC14_PROCC\", \"PUFC16_PKB\", \"PUFC17_NATEM\",\n",
    "    \"PUFC18_PNWHRS\", \"PUFC19_PHOURS\",\n",
    "    \"PUFC20_PWMORE\", \"PUFC21_PLADDW\", \"PUFC22_PFWRK\", \"PUFC23_PCLASS\",\n",
    "    \"PUFC24_PBASIS\", \"PUFC25_PBASIC\",\n",
    "    \"PUFC26_OJOB\",\n",
    "    \"PUFC27_NJOBS\", \"PUFC28_THOURS\", \"PUFC29_WWM48H\",\n",
    "    \"PUFC30_LOOKW\", \"PUFC31_FLWRK\", \"PUFC32_JOBSM\", \"PUFC33_WEEKS\",\n",
    "    \"PUFC34_WYNOT\", \"PUFC35_LTLOOKW\", \"PUFC36_AVAIL\", \"PUFC37_WILLING\",\n",
    "    \"PUFC38_PREVJOB\", \"PUFC39_YEAR\", \"PUFC39_MONTH\", \"PUFC41_POCC\", \"PUFC43_QKB\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d5c176",
   "metadata": {},
   "source": [
    "## 3. Implement Utility and Translation Functions\n",
    "Helpers for parsing, column selection, and year-specific code translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08d3ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def safe_numeric(x):\n",
    "    if x is None:\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        return float(x)\n",
    "    if isinstance(x, (float, np.floating)):\n",
    "        return x if not np.isnan(x) else np.nan\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if x == \"\" or x.lower() in [\"nan\", \"na\", \".\", \"none\"]:\n",
    "            return np.nan\n",
    "        try:\n",
    "            return float(x)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def safe_int(x):\n",
    "    val = safe_numeric(x)\n",
    "    return np.nan if pd.isna(val) else int(val)\n",
    "\n",
    "\n",
    "def clean_column(series):\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def get_column(df, target, col_map_upper):\n",
    "    for src in COLUMN_PRIORITY.get(target, [target]):\n",
    "        if src.upper() in col_map_upper:\n",
    "            col = df[col_map_upper[src.upper()]].copy()\n",
    "            non_empty = col.dropna()\n",
    "            if len(non_empty) > 0:\n",
    "                if col.dtype == object:\n",
    "                    non_whitespace = non_empty[non_empty.astype(str).str.strip() != \"\"]\n",
    "                    if len(non_whitespace) > 0:\n",
    "                        return col\n",
    "                else:\n",
    "                    return col\n",
    "    return None\n",
    "\n",
    "\n",
    "def translate_mstat(code, year):\n",
    "    code = safe_int(code)\n",
    "    if pd.isna(code):\n",
    "        return np.nan\n",
    "    if year <= 2010:\n",
    "        return {1: 1, 2: 2, 3: 4, 4: 6, 5: 8}.get(code, np.nan)\n",
    "    if year <= 2014:\n",
    "        return {1: 1, 2: 2, 3: 4, 4: 6, 5: 8, 6: 7}.get(code, np.nan)\n",
    "    if year <= 2023:\n",
    "        return {1: 1, 2: 2, 3: 4, 4: 6, 5: 7, 6: 8}.get(code, np.nan)\n",
    "    return code\n",
    "\n",
    "\n",
    "def translate_grade(code, year):\n",
    "    \"\"\"Map education grade codes to PSCED 2017 levels 0-8.\n",
    "\n",
    "    PSCED 2017 Levels:\n",
    "        0 = Early Childhood Education / No Grade Completed\n",
    "        1 = Primary Education\n",
    "        2 = Lower Secondary Education\n",
    "        3 = Upper Secondary Education\n",
    "        4 = Post-Secondary Non-Tertiary Education\n",
    "        5 = Short-Cycle Tertiary Education or Equivalent\n",
    "        6 = Bachelor Level Education or Equivalent\n",
    "        7 = Master Level Education or Equivalent\n",
    "        8 = Doctoral Level Education or Equivalent\n",
    "\n",
    "    Raw code schemes vary by era:\n",
    "        <=2011  : Old LFS codes 0-78\n",
    "        2012-2018: K-12 transition 3-digit codes 0-940\n",
    "            NOTE: 801-899 are bachelor's degree field codes (NOT doctoral).\n",
    "            The 3-digit coding uses 6xx AND 8xx for bachelor's fields.\n",
    "        2019+   : Full PSCED 5-digit codes (e.g. 10011, 60413)\n",
    "            Here 8xxxx ARE doctoral (PSCED Level 8).\n",
    "    \"\"\"\n",
    "    code = safe_int(code)\n",
    "    if pd.isna(code):\n",
    "        return np.nan\n",
    "\n",
    "    # ---- Era 1: Pre-K12 system (<=2011) ----\n",
    "    if year <= 2011:\n",
    "        if code == 0:\n",
    "            return 0                    # No grade completed\n",
    "        if code in (1, 2):\n",
    "            return 1                    # Elementary undergrad/graduate -> Primary\n",
    "        if code == 3:\n",
    "            return 2                    # High school undergrad -> Lower Secondary\n",
    "        if code == 4:\n",
    "            return 3                    # High school graduate -> Upper Secondary\n",
    "        if code == 5:\n",
    "            return 5                    # Post-secondary / vocational -> Short-Cycle Tertiary\n",
    "        if 60 <= code <= 68:\n",
    "            return 5                    # College undergrad -> Short-Cycle Tertiary\n",
    "        if 70 <= code <= 76:\n",
    "            return 6                    # College graduate -> Bachelor's\n",
    "        if code == 78:\n",
    "            return 7                    # Post-baccalaureate -> Master's\n",
    "        return np.nan\n",
    "\n",
    "    # ---- Era 2: K-12 transition 3-digit codes (2012-2018) ----\n",
    "    # NOTE: In this coding scheme, 801-899 are additional bachelor's\n",
    "    # field-of-study codes, NOT doctoral programs. The 3-digit era has\n",
    "    # no explicit doctoral code â€” only master's (710-760).\n",
    "    if year <= 2018:\n",
    "        if code == 0:\n",
    "            return 0                    # No grade completed\n",
    "        if code in (1, 2, 10):\n",
    "            return 0                    # Pre-school / Kindergarten\n",
    "        if 110 <= code <= 192:\n",
    "            return 1                    # Elementary Gr1-6, grad, ALS primary\n",
    "        if 210 <= code <= 270:\n",
    "            return 2                    # JHS / HS undergrad (old 4-yr HS)\n",
    "        if code == 280:\n",
    "            return 3                    # HS Graduate (old 4-year system)\n",
    "        if 310 <= code <= 350:\n",
    "            return 3                    # SHS grades / SHS Graduate\n",
    "        if 410 <= code <= 499:\n",
    "            return 4                    # Post-secondary non-tertiary\n",
    "        if 500 <= code <= 559:\n",
    "            return 5                    # Short-cycle tertiary / TVET\n",
    "        if 560 <= code <= 599:\n",
    "            return 5                    # College undergrad (pursuing bachelor's)\n",
    "        if 601 <= code <= 699:\n",
    "            return 6                    # Bachelor's degree (field set A)\n",
    "        if 710 <= code <= 760:\n",
    "            return 7                    # Master's degree\n",
    "        if 801 <= code <= 899:\n",
    "            return 6                    # Bachelor's degree (field set B)\n",
    "        # 900-940: Other/unspecified education -> NaN\n",
    "        return np.nan\n",
    "\n",
    "    # ---- Era 3: Full PSCED 5-digit codes (2019+) ----\n",
    "    if code == 0:\n",
    "        return 0                        # No grade completed\n",
    "    if code == 1000:\n",
    "        return 0                        # Early childhood / Kindergarten\n",
    "    if code == 2000:\n",
    "        return 1                        # Basic education placeholder -> Primary\n",
    "\n",
    "    # Extract PSCED level from first digit of 5-digit code\n",
    "    if 10000 <= code <= 19999:\n",
    "        return 1                        # Level 1: Primary Education\n",
    "    if 20000 <= code <= 29999:\n",
    "        return 2                        # Level 2: Lower Secondary\n",
    "    if 30000 <= code <= 39999:\n",
    "        return 3                        # Level 3: Upper Secondary\n",
    "    if 40000 <= code <= 49999:\n",
    "        return 4                        # Level 4: Post-Secondary Non-Tertiary\n",
    "    if 50000 <= code <= 59999:\n",
    "        return 5                        # Level 5: Short-Cycle Tertiary\n",
    "    if 60000 <= code <= 69999:\n",
    "        return 6                        # Level 6: Bachelor's\n",
    "    if 70000 <= code <= 79999:\n",
    "        return 7                        # Level 7: Master's\n",
    "    if 80000 <= code <= 89999:\n",
    "        return 8                        # Level 8: Doctoral\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def translate_pclass(code, year):\n",
    "    code = safe_int(code)\n",
    "    if pd.isna(code):\n",
    "        return np.nan\n",
    "    return code if code in [0, 1, 2, 3, 4, 5, 6] else np.nan\n",
    "\n",
    "\n",
    "def translate_natem(code, year):\n",
    "    code = safe_int(code)\n",
    "    if pd.isna(code):\n",
    "        return np.nan\n",
    "    return code if code in [1, 2, 3] else np.nan\n",
    "\n",
    "\n",
    "def translate_pbasis(code, year):\n",
    "    code = safe_int(code)\n",
    "    if pd.isna(code):\n",
    "        return np.nan\n",
    "    return code if code in [0, 1, 2, 3, 4, 5, 6, 7] else np.nan\n",
    "\n",
    "\n",
    "def translate_wynot(code, year):\n",
    "    code = safe_int(code)\n",
    "    if pd.isna(code):\n",
    "        return np.nan\n",
    "    if year < 2021:\n",
    "        if code == 6:\n",
    "            return 61\n",
    "        return code if code in [0, 1, 2, 3, 4, 5, 7, 8, 9] else np.nan\n",
    "    return code\n",
    "\n",
    "\n",
    "def translate_conwr(code, year):\n",
    "    code = safe_int(code)\n",
    "    if pd.isna(code):\n",
    "        return np.nan\n",
    "    return code if code in [1, 2, 3, 4, 5] else np.nan\n",
    "\n",
    "\n",
    "def translate_yesno(code, year):\n",
    "    code = safe_int(code)\n",
    "    if pd.isna(code):\n",
    "        return np.nan\n",
    "    return code if code in [1, 2] else np.nan\n",
    "\n",
    "\n",
    "def translate_rel(code, year):\n",
    "    code = safe_int(code)\n",
    "    if pd.isna(code):\n",
    "        return np.nan\n",
    "    return code if 1 <= code <= 26 else np.nan\n",
    "\n",
    "\n",
    "TRANSLATION_MAP = {\n",
    "    \"PUFC03_REL\": translate_rel,\n",
    "    \"PUFC06_MSTAT\": translate_mstat,\n",
    "    \"PUFC07_GRADE\": translate_grade,\n",
    "    \"PUFC10_CONWR\": translate_conwr,\n",
    "    \"PUFC17_NATEM\": translate_natem,\n",
    "    \"PUFC20_PWMORE\": translate_yesno,\n",
    "    \"PUFC21_PLADDW\": translate_yesno,\n",
    "    \"PUFC22_PFWRK\": translate_yesno,\n",
    "    \"PUFC23_PCLASS\": translate_pclass,\n",
    "    \"PUFC24_PBASIS\": translate_pbasis,\n",
    "    \"PUFC26_OJOB\": translate_yesno,\n",
    "    \"PUFC30_LOOKW\": translate_yesno,\n",
    "    \"PUFC31_FLWRK\": translate_yesno,\n",
    "    \"PUFC34_WYNOT\": translate_wynot,\n",
    "    \"PUFC36_AVAIL\": translate_yesno,\n",
    "    \"PUFC37_WILLING\": translate_yesno,\n",
    "    \"PUFC38_PREVJOB\": translate_yesno,\n",
    "    \"PUFC11_WORK\": translate_yesno,\n",
    "    \"PUFC12_JOB\": translate_yesno,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183ebef",
   "metadata": {},
   "source": [
    "## 4. Process a Single CSV File\n",
    "Read one CSV, detect year/month, map columns, apply translations, and return a harmonized DataFrame plus per-file diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4090d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def extract_year_month(filepath):\n",
    "    filename = Path(filepath).stem.upper()\n",
    "    month_map = {\n",
    "        \"JAN\": 1, \"FEB\": 2, \"MAR\": 3, \"APR\": 4, \"MAY\": 5, \"JUN\": 6,\n",
    "        \"JUL\": 7, \"AUG\": 8, \"SEP\": 9, \"OCT\": 10, \"NOV\": 11, \"DEC\": 12,\n",
    "    }\n",
    "    year_match = re.search(r\"(20\\d{2}|199\\d)\", filename)\n",
    "    year = int(year_match.group(1)) if year_match else None\n",
    "    month = next((m_num for m_name, m_num in month_map.items() if m_name in filename), None)\n",
    "    return year, month\n",
    "\n",
    "\n",
    "def read_csv(filepath):\n",
    "    na_vals = [\"\", \"\\t\", \" \", \"  \", \"   \", \".\", \"NA\", \"nan\", \"NaN\", \"N/A\"]\n",
    "    for enc in [\"utf-8\", \"latin-1\", \"cp1252\"]:\n",
    "        try:\n",
    "            return pd.read_csv(filepath, encoding=enc, low_memory=False, na_values=na_vals)\n",
    "        except pd.errors.ParserError:\n",
    "            try:\n",
    "                return pd.read_csv(\n",
    "                    filepath,\n",
    "                    encoding=enc,\n",
    "                    na_values=na_vals,\n",
    "                    engine=\"python\",\n",
    "                    on_bad_lines=\"warn\",\n",
    "                )\n",
    "            except Exception:\n",
    "                continue\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.read_csv(filepath, encoding=\"utf-8\", engine=\"python\", on_bad_lines=\"warn\")\n",
    "\n",
    "\n",
    "def get_column_with_source(df, target, col_map_upper):\n",
    "    \"\"\"Like get_column but also returns which source column name was used.\"\"\"\n",
    "    for src in COLUMN_PRIORITY.get(target, [target]):\n",
    "        if src.upper() in col_map_upper:\n",
    "            col = df[col_map_upper[src.upper()]].copy()\n",
    "            non_empty = col.dropna()\n",
    "            if len(non_empty) > 0:\n",
    "                if col.dtype == object:\n",
    "                    non_whitespace = non_empty[non_empty.astype(str).str.strip() != \"\"]\n",
    "                    if len(non_whitespace) > 0:\n",
    "                        return col, col_map_upper[src.upper()]\n",
    "                else:\n",
    "                    return col, col_map_upper[src.upper()]\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def process_file(filepath, log_messages):\n",
    "    \"\"\"Process a single CSV and return (DataFrame, diagnostics_dict).\n",
    "    \n",
    "    Returns (None, error_diagnostics) on read failure.\n",
    "    \"\"\"\n",
    "    fname = Path(filepath).name\n",
    "\n",
    "    def log(msg):\n",
    "        print(msg)\n",
    "        log_messages.append(msg)\n",
    "\n",
    "    log(f\"Processing: {fname}\")\n",
    "\n",
    "    try:\n",
    "        df = read_csv(filepath)\n",
    "    except Exception as exc:\n",
    "        log(f\"  ERROR reading file: {exc}\")\n",
    "        return None, {\"file_name\": fname, \"error\": str(exc)}\n",
    "\n",
    "    year, month = extract_year_month(filepath)\n",
    "    col_map_upper = {c.upper(): c for c in df.columns}\n",
    "\n",
    "    if year is None:\n",
    "        for col in [\"SVYYR\", \"CYEAR\", \"PUFSVYYR\"]:\n",
    "            if col.upper() in col_map_upper:\n",
    "                try:\n",
    "                    year = int(pd.to_numeric(df[col_map_upper[col.upper()]], errors=\"coerce\").mode().iloc[0])\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if year:\n",
    "                    break\n",
    "        if year is None:\n",
    "            log(\"  WARNING: Could not detect year, defaulting to 2020\")\n",
    "            year = 2020\n",
    "\n",
    "    log(f\"  Year: {year}, Month: {month}, Rows: {len(df):,}, Cols: {len(df.columns)}\")\n",
    "\n",
    "    # --- Build harmonized output with diagnostics ---\n",
    "    out_df = pd.DataFrame(index=df.index)\n",
    "    missing_cols = []\n",
    "    source_column_used = {}\n",
    "    translation_drops = {}\n",
    "    dropped_code_values = {}\n",
    "\n",
    "    # Track which raw columns get used\n",
    "    used_raw_columns = set()\n",
    "\n",
    "    # Also build set of ALL raw column names that appear in any COLUMN_PRIORITY list\n",
    "    all_known_sources_upper = set()\n",
    "    for sources in COLUMN_PRIORITY.values():\n",
    "        for s in sources:\n",
    "            all_known_sources_upper.add(s.upper())\n",
    "\n",
    "    for target in OUTPUT_SCHEMA:\n",
    "        col_data, src_name = get_column_with_source(df, target, col_map_upper)\n",
    "        if col_data is not None:\n",
    "            source_column_used[target] = src_name\n",
    "            used_raw_columns.add(src_name.upper())\n",
    "            col_data = clean_column(col_data)\n",
    "\n",
    "            if target in TRANSLATION_MAP:\n",
    "                # Count non-null before translation\n",
    "                pre_valid = col_data.notna().sum()\n",
    "                translated = col_data.apply(lambda x: TRANSLATION_MAP[target](x, year))\n",
    "                post_valid = translated.notna().sum()\n",
    "                drops = int(pre_valid - post_valid)\n",
    "                translation_drops[target] = drops\n",
    "\n",
    "                # Capture the actual code values that got dropped\n",
    "                if drops > 0:\n",
    "                    mask_dropped = col_data.notna() & translated.isna()\n",
    "                    dropped_vals = col_data[mask_dropped].dropna()\n",
    "                    if len(dropped_vals) > 0:\n",
    "                        vc = dropped_vals.value_counts().head(10)\n",
    "                        dropped_code_values[target] = {str(int(k)): int(v) for k, v in vc.items()}\n",
    "\n",
    "                out_df[target] = translated\n",
    "            else:\n",
    "                out_df[target] = col_data\n",
    "        else:\n",
    "            out_df[target] = np.nan\n",
    "            missing_cols.append(target)\n",
    "            source_column_used[target] = None\n",
    "\n",
    "    # Log all missing columns (not truncated)\n",
    "    if missing_cols:\n",
    "        log(f\"  Missing columns ({len(missing_cols)}): {', '.join(missing_cols)}\")\n",
    "\n",
    "    out_df[\"PUFSVYYR\"] = year\n",
    "    if month:\n",
    "        out_df[\"PUFSVYMO\"] = month\n",
    "\n",
    "    out_df = out_df[OUTPUT_SCHEMA]\n",
    "\n",
    "    # --- Compute diagnostics ---\n",
    "    # Unmapped raw columns: columns in the CSV that are NOT in any COLUMN_PRIORITY list\n",
    "    unmapped_raw = [c for c in df.columns if c.upper() not in all_known_sources_upper]\n",
    "\n",
    "    # Null rates\n",
    "    null_rates = {}\n",
    "    for col in OUTPUT_SCHEMA:\n",
    "        total = len(out_df)\n",
    "        nulls = int(out_df[col].isna().sum())\n",
    "        null_rates[col] = round(nulls / total * 100, 2) if total > 0 else 0.0\n",
    "\n",
    "    # Value ranges for numeric columns\n",
    "    value_ranges = {}\n",
    "    for col in OUTPUT_SCHEMA:\n",
    "        s = out_df[col].dropna()\n",
    "        if len(s) > 0:\n",
    "            value_ranges[col] = {\n",
    "                \"min\": float(s.min()),\n",
    "                \"max\": float(s.max()),\n",
    "                \"n_unique\": int(s.nunique()),\n",
    "            }\n",
    "\n",
    "    # Weight statistics\n",
    "    weight_stats = {}\n",
    "    if \"PUFPWGTPRV\" in out_df.columns:\n",
    "        w = out_df[\"PUFPWGTPRV\"].dropna()\n",
    "        if len(w) > 0:\n",
    "            weight_stats = {\n",
    "                \"min\": float(w.min()),\n",
    "                \"max\": float(w.max()),\n",
    "                \"mean\": round(float(w.mean()), 2),\n",
    "                \"median\": float(w.median()),\n",
    "                \"zero_count\": int((w == 0).sum()),\n",
    "                \"negative_count\": int((w < 0).sum()),\n",
    "                \"sum\": float(w.sum()),\n",
    "            }\n",
    "\n",
    "    # Sex distribution\n",
    "    sex_dist = {}\n",
    "    if \"PUFC04_SEX\" in out_df.columns:\n",
    "        vc = out_df[\"PUFC04_SEX\"].dropna().value_counts()\n",
    "        sex_dist = {str(int(k)): int(v) for k, v in vc.items()}\n",
    "\n",
    "    # Age statistics\n",
    "    age_stats = {}\n",
    "    if \"PUFC05_AGE\" in out_df.columns:\n",
    "        ages = out_df[\"PUFC05_AGE\"].dropna()\n",
    "        if len(ages) > 0:\n",
    "            age_stats = {\n",
    "                \"min\": float(ages.min()),\n",
    "                \"max\": float(ages.max()),\n",
    "                \"mean\": round(float(ages.mean()), 2),\n",
    "                \"implausible_negative\": int((ages < 0).sum()),\n",
    "                \"implausible_over_120\": int((ages > 120).sum()),\n",
    "            }\n",
    "\n",
    "    # Translation drop summary for log\n",
    "    if translation_drops:\n",
    "        drops_nonzero = {k: v for k, v in translation_drops.items() if v > 0}\n",
    "        if drops_nonzero:\n",
    "            log(f\"  Translation drops: {drops_nonzero}\")\n",
    "\n",
    "    diagnostics = {\n",
    "        \"file_name\": fname,\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"raw_rows\": len(df),\n",
    "        \"raw_cols\": len(df.columns),\n",
    "        \"harmonized_rows\": len(out_df),\n",
    "        \"mapped_count\": len(OUTPUT_SCHEMA) - len(missing_cols),\n",
    "        \"missing_count\": len(missing_cols),\n",
    "        \"source_column_used\": source_column_used,\n",
    "        \"unmapped_raw_columns\": unmapped_raw,\n",
    "        \"missing_target_columns\": missing_cols,\n",
    "        \"null_rates\": null_rates,\n",
    "        \"translation_drops\": translation_drops,\n",
    "        \"dropped_code_values\": dropped_code_values,\n",
    "        \"value_ranges\": value_ranges,\n",
    "        \"weight_stats\": weight_stats,\n",
    "        \"sex_distribution\": sex_dist,\n",
    "        \"age_stats\": age_stats,\n",
    "    }\n",
    "\n",
    "    log(f\"  Output: {len(out_df.columns)} columns, {len(out_df):,} rows [OK]\")\n",
    "    return out_df, diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b10474",
   "metadata": {},
   "source": [
    "## 5. Batch Process Directory and Write Parquet Outputs\n",
    "Iterate over input files, write per-file outputs, and merge in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e80470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_batched(input_dir, output_dir, batch_size=10):\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    individual_dir = output_path / \"individual_files\"\n",
    "    individual_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    all_files = list(input_path.glob(\"*.csv\")) + list(input_path.glob(\"*.CSV\"))\n",
    "    seen = set()\n",
    "    files = []\n",
    "    for f in sorted(all_files, key=lambda x: x.name.lower()):\n",
    "        name_lower = f.name.lower()\n",
    "        if name_lower not in seen:\n",
    "            seen.add(name_lower)\n",
    "            files.append(f)\n",
    "\n",
    "    log_messages = []\n",
    "    errors = []\n",
    "    all_parquet_files = []\n",
    "    all_diagnostics = []\n",
    "    total_rows = 0\n",
    "\n",
    "    def log(msg):\n",
    "        print(msg)\n",
    "        log_messages.append(msg)\n",
    "\n",
    "    log(\"=\" * 60)\n",
    "    log(\"LFS HARMONIZER v8 - 47 CORE COLUMNS (with validation)\")\n",
    "    log(f\"Started: {datetime.now().isoformat()}\")\n",
    "    log(f\"Files found: {len(files)}\")\n",
    "    log(f\"Batch size: {batch_size}\")\n",
    "    log(\"=\" * 60)\n",
    "\n",
    "    for i, f in enumerate(files, 1):\n",
    "        log(f\"\\n[{i}/{len(files)}]\")\n",
    "        try:\n",
    "            result = process_file(str(f), log_messages)\n",
    "            df, diag = result\n",
    "            all_diagnostics.append(diag)\n",
    "            if df is not None:\n",
    "                out_name = f\"{f.stem}_harmonized.parquet\"\n",
    "                out_file = individual_dir / out_name\n",
    "                df.to_parquet(out_file, index=False, compression=\"snappy\")\n",
    "                all_parquet_files.append(out_file)\n",
    "                total_rows += len(df)\n",
    "                del df\n",
    "                gc.collect()\n",
    "        except Exception as exc:\n",
    "            log(f\"  FATAL ERROR: {exc}\")\n",
    "            errors.append((f.name, str(exc)))\n",
    "\n",
    "    log(f\"\\n{'=' * 60}\")\n",
    "    log(\"INDIVIDUAL FILES COMPLETE\")\n",
    "    log(f\"  Processed: {len(all_parquet_files)} files\")\n",
    "    log(f\"  Errors: {len(errors)} files\")\n",
    "    log(f\"  Total rows: {total_rows:,}\")\n",
    "    log(\"=\" * 60)\n",
    "\n",
    "    combined_file = None\n",
    "    if all_parquet_files:\n",
    "        log(f\"\\nCombining files in batches of {batch_size}...\")\n",
    "        combined_file = output_path / \"lfs_harmonized_2024codes.parquet\"\n",
    "\n",
    "        first_batch = True\n",
    "        for batch_start in range(0, len(all_parquet_files), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(all_parquet_files))\n",
    "            batch_files = all_parquet_files[batch_start:batch_end]\n",
    "\n",
    "            log(f\"  Batch {batch_start // batch_size + 1}: files {batch_start + 1}-{batch_end}\")\n",
    "\n",
    "            batch_dfs = [pd.read_parquet(f) for f in batch_files]\n",
    "            batch_combined = pd.concat(batch_dfs, ignore_index=True)\n",
    "            del batch_dfs\n",
    "            gc.collect()\n",
    "\n",
    "            if first_batch:\n",
    "                batch_combined.to_parquet(combined_file, index=False, compression=\"snappy\")\n",
    "                first_batch = False\n",
    "            else:\n",
    "                existing = pd.read_parquet(combined_file)\n",
    "                combined = pd.concat([existing, batch_combined], ignore_index=True)\n",
    "                combined.to_parquet(combined_file, index=False, compression=\"snappy\")\n",
    "                del existing, combined\n",
    "\n",
    "            del batch_combined\n",
    "            gc.collect()\n",
    "\n",
    "        log(\"\\nFinal sorting by year/month...\")\n",
    "        final_df = pd.read_parquet(combined_file)\n",
    "        if \"PUFSVYYR\" in final_df.columns:\n",
    "            final_df = final_df.sort_values([\"PUFSVYYR\", \"PUFSVYMO\"]).reset_index(drop=True)\n",
    "        final_df.to_parquet(combined_file, index=False, compression=\"snappy\")\n",
    "\n",
    "        log(f\"\\n{'=' * 60}\")\n",
    "        log(\"HARMONIZATION COMPLETE!\")\n",
    "        log(\"=\" * 60)\n",
    "        log(f\"Output: {combined_file}\")\n",
    "        log(f\"Total rows: {len(final_df):,}\")\n",
    "        log(f\"Columns: {len(final_df.columns)}\")\n",
    "        if \"PUFSVYYR\" in final_df.columns:\n",
    "            log(f\"Years: {int(final_df['PUFSVYYR'].min())} - {int(final_df['PUFSVYYR'].max())}\")\n",
    "\n",
    "        meta = {\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"version\": \"v8-notebook\",\n",
    "            \"files_processed\": len(all_parquet_files),\n",
    "            \"files_with_errors\": len(errors),\n",
    "            \"total_rows\": len(final_df),\n",
    "            \"columns\": list(final_df.columns),\n",
    "            \"column_count\": len(final_df.columns),\n",
    "            \"errors\": errors[:20],\n",
    "        }\n",
    "        with open(output_path / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "\n",
    "        del final_df\n",
    "        gc.collect()\n",
    "    else:\n",
    "        meta = {\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"version\": \"v8-notebook\",\n",
    "            \"files_processed\": 0,\n",
    "            \"files_with_errors\": len(errors),\n",
    "            \"total_rows\": 0,\n",
    "            \"columns\": [],\n",
    "            \"column_count\": 0,\n",
    "            \"errors\": errors[:20],\n",
    "        }\n",
    "        with open(output_path / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "\n",
    "    log_path = output_path / \"harmonization_log.txt\"\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(log_messages))\n",
    "\n",
    "    # Save raw diagnostics as JSON for the validation report\n",
    "    diag_path = output_path / \"file_diagnostics.json\"\n",
    "    with open(diag_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_diagnostics, f, indent=2, default=str)\n",
    "\n",
    "    return combined_file, meta, log_path, all_diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff496c80",
   "metadata": {},
   "source": [
    "## 6. Summarize Metadata and Logs\n",
    "Run the harmonizer and write metadata and logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a7bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LFS HARMONIZER v8 - 47 CORE COLUMNS (with validation)\n",
      "Started: 2026-02-12T07:26:13.290356\n",
      "Files found: 105\n",
      "Batch size: 5\n",
      "============================================================\n",
      "\n",
      "[1/105]\n",
      "Processing: 2005-01JAN.CSV\n",
      "  Year: 2005, Month: 1, Rows: 211,888, Cols: 85\n"
     ]
    }
   ],
   "source": [
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_messages = []\n",
    "combined_file = None\n",
    "meta = {}\n",
    "all_diagnostics = []\n",
    "log_path = output_dir / \"harmonization_log.txt\"\n",
    "\n",
    "if single_file:\n",
    "    result = process_file(single_file, log_messages)\n",
    "    df, diag = result\n",
    "    all_diagnostics.append(diag)\n",
    "    if df is not None:\n",
    "        out_file = output_dir / f\"{Path(single_file).stem}_harmonized.parquet\"\n",
    "        df.to_parquet(out_file, index=False, compression=\"snappy\")\n",
    "        combined_file = out_file\n",
    "        meta = {\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"version\": \"v8-notebook\",\n",
    "            \"files_processed\": 1,\n",
    "            \"files_with_errors\": 0,\n",
    "            \"total_rows\": len(df),\n",
    "            \"columns\": list(df.columns),\n",
    "            \"column_count\": len(df.columns),\n",
    "            \"errors\": [],\n",
    "        }\n",
    "        if show_samples:\n",
    "            display(df.head())\n",
    "\n",
    "    with open(output_dir / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(log_messages) if log_messages else \"\")\n",
    "else:\n",
    "    combined_file, meta, log_path, all_diagnostics = process_all_batched(\n",
    "        input_dir, output_dir, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "print(\"Metadata:\")\n",
    "print(json.dumps(meta, indent=2))\n",
    "print(f\"Log saved to: {log_path}\")\n",
    "\n",
    "if combined_file and Path(combined_file).exists():\n",
    "    try:\n",
    "        cols = [\"PUFSVYYR\"] if \"PUFSVYYR\" in meta.get(\"columns\", []) else None\n",
    "        if cols:\n",
    "            years = pd.read_parquet(combined_file, columns=cols)[\"PUFSVYYR\"].dropna()\n",
    "            if not years.empty:\n",
    "                print(f\"Year range: {int(years.min())} - {int(years.max())}\")\n",
    "        if show_samples and not single_file:\n",
    "            sample_df = pd.read_parquet(combined_file).head()\n",
    "            display(sample_df)\n",
    "    except Exception as exc:\n",
    "        print(f\"Could not summarize parquet: {exc}\")\n",
    "else:\n",
    "    print(\"No output parquet produced.\")\n",
    "\n",
    "print(f\"\\nDiagnostics collected for {len(all_diagnostics)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y4c8rmn3shh",
   "metadata": {},
   "source": [
    "## 7. Validation Report\n",
    "Generate comprehensive validation outputs from the per-file diagnostics collected during harmonization. Produces:\n",
    "- **Per-file summary table** (`per_file_diagnostics.csv`)\n",
    "- **Unmapped variables list** (`unmapped_variables.csv`)\n",
    "- **Missing columns matrix** (`missing_columns_matrix.csv`)\n",
    "- **Translation drop details** (`translation_drops.csv`)\n",
    "- **Human-readable validation report** (`validation_report.txt`)\n",
    "- **Machine-readable report** (`validation_report.json`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sr0x1ay5bx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation report saved to: output_v8\\validation_report.txt\n",
      "JSON report saved to: output_v8\\validation_report.json\n",
      "Per-file diagnostics: output_v8\\per_file_diagnostics.csv\n",
      "Unmapped variables: output_v8\\unmapped_variables.csv\n",
      "Missing columns matrix: output_v8\\missing_columns_matrix.csv\n",
      "Translation drops: output_v8\\translation_drops.csv\n",
      "\n",
      "Total flags raised: 4\n",
      "======================================================================\n",
      "LFS HARMONIZATION - VALIDATION REPORT\n",
      "Generated: 2026-02-12T04:27:00.054828\n",
      "Files analyzed: 105 successful, 0 errors\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "A. PER-FILE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Saved: per_file_diagnostics.csv (105 rows)\n",
      "\n",
      "file                           year mo   raw_rows mapped miss  null% t_drops\n",
      "---------------------------------------------------------------------------\n",
      "2005-01JAN.CSV                 2005  1    211,888     35   13   58.2       0\n",
      "2005-04APR.CSV                 2005  4    213,475     43    5   56.4       0\n",
      "2005-07JUL.CSV                 2005  7    206,467     43    5   56.3       2\n",
      "2005-10OCT.CSV                 2005 10    204,459     43    5   56.2       0\n",
      "2006-01JAN.CSV                 2006  1    203,795     43    5   56.3       0\n",
      "2006-04APR.CSV                 2006  4    206,373     43    5   56.2       0\n",
      "2006-07JUL.CSV                 2006  7    200,717     43    5   56.2       0\n",
      "2006-10OCT.CSV                 2006 10    201,134     43    5   56.1       0\n",
      "2007-01JAN.CSV                 2007  1    198,627     43    5   56.1       0\n",
      "2007-04APR.CSV                 2007  4    202,256     43    5   56.0       0\n",
      "2007-07JUL.CSV                 2007  7    196,085     43    5   56.0       0\n",
      "2007-10OCT.CSV                 2007 10    201,872     43    5   56.1       0\n",
      "2008-01JAN.CSV                 2008  1    202,168     43    5   56.0       0\n",
      "2008-04APR.CSV                 2008  4    197,929     43    5   56.0       0\n",
      "2008-07JUL.CSV                 2008  7    195,566     43    5   55.9       0\n",
      "2008-10OCT.CSV                 2008 10    202,083     43    5   56.0       0\n",
      "2009-01JAN.CSV                 2009  1    202,132     43    5   56.0      18\n",
      "2009-04APR.CSV                 2009  4    201,686     43    5   55.8      17\n",
      "2009-07JUL.CSV                 2009  7    198,870     43    5   55.7      13\n",
      "2009-10OCT.CSV                 2009 10    201,478     43    5   55.8      13\n",
      "2010-01JAN.CSV                 2010  1    195,850     43    5   55.7      15\n",
      "2010-04APR.CSV                 2010  4    202,479     43    5   55.6      12\n",
      "2010-07JUL.CSV                 2010  7    195,827     43    5   55.6      14\n",
      "2010-10OCT.CSV                 2010 10    201,695     43    5   55.7      11\n",
      "2011-01JAN.CSV                 2011  1    203,948     43    5   55.7       0\n",
      "2011-04APR.CSV                 2011  4    201,917     43    5   55.5       0\n",
      "2011-07JUL.CSV                 2011  7    198,755     43    5   55.6       0\n",
      "2011-10OCT.CSV                 2011 10    203,011     43    5   55.3       0\n",
      "2012-01JAN.CSV                 2012  1    204,725     45    3   53.0     455\n",
      "2012-04APR.CSV                 2012  4    202,696     45    3   52.9     360\n",
      "2012-07JUL.CSV                 2012  7    203,611     45    3   51.9     471\n",
      "2012-10OCT.CSV                 2012 10    206,020     45    3   52.1     376\n",
      "2013-01JAN.CSV                 2013  1    201,620     44    4   54.1     505\n",
      "2013-04APR.CSV                 2013  4    202,801     44    4   53.9     420\n",
      "2013-07JUL.CSV                 2013  7    206,585     44    4   53.9     398\n",
      "2013-10OCT.CSV                 2013 10    207,094     44    4   54.0     426\n",
      "2014-01JAN.CSV                 2014  1    201,545     44    4   54.1     456\n",
      "2014-04APR.CSV                 2014  4    201,365     44    4   53.8     504\n",
      "2014-07JUL.CSV                 2014  7    200,324     44    4   53.9     471\n",
      "2014-10OCT.CSV                 2014 10    202,040     44    4   53.9     406\n",
      "2015-01JAN.CSV                 2015  1    204,564     44    4   54.0     497\n",
      "2015-04APR.CSV                 2015  4    201,487     44    4   53.8     550\n",
      "2015-07JUL.CSV                 2015  7    207,102     44    4   54.0     573\n",
      "2015-10OCT.CSV                 2015 10    207,641     44    4   53.9     442\n",
      "2016-01JAN.CSV                 2016  1    207,188     44    4   53.9     611\n",
      "2016-04APR.CSV                 2016  4    180,862     46    2   50.1     420\n",
      "2016-07JUL.CSV                 2016  7    182,904     46    2   50.1     480\n",
      "2016-10OCT.CSV                 2016 10    181,271     46    2   50.0     387\n",
      "2017-01JAN.CSV                 2017  1    182,824     46    2   50.5     681\n",
      "2017-04APR.CSV                 2017  4    179,525     46    2   50.3     588\n",
      "2017-07JUL.CSV                 2017  7    179,595     46    2   50.2     775\n",
      "2017-10OCT.CSV                 2017 10    179,007     46    2   49.9     721\n",
      "2018-01JAN.CSV                 2018  1    180,262     46    2   50.0     600\n",
      "2018-04APR.CSV                 2018  4    179,815     45    3   51.9     628\n",
      "2018-07JUL.CSV                 2018  7    182,956     46    2   50.0     754\n",
      "2018-10OCT.CSV                 2018 10    179,204     46    2   49.7     629\n",
      "2019-01JAN.CSV                 2019  1    181,233     46    2   49.8       0\n",
      "2019-04APR.CSV                 2019  4    172,284     46    2   49.4       0\n",
      "2019-07JUL.CSV                 2019  7    175,438     46    2   49.4       0\n",
      "2019-10OCT.CSV                 2019 10    178,067     46    2   49.3       0\n",
      "2020-01JAN.CSV                 2020  1    178,140     46    2   49.2       0\n",
      "2020-04APR.CSV                 2020  4    176,469     46    2   49.8       0\n",
      "2020-07JUL.CSV                 2020  7    176,355     46    2   49.1       0\n",
      "2020-10OCT.csv                 2020 10    185,419     46    2   49.4       0\n",
      "2021-01JAN.csv                 2021  1    188,212     46    2   50.4  109499\n",
      "2021-02FEB.csv                 2021  2     47,930     37   11   53.0     270\n",
      "2021-03MAR.csv                 2021  3     47,606     37   11   52.9     205\n",
      "2021-04APR.csv                 2021  4    188,664     48    0   48.5    1455\n",
      "2021-05MAY.csv                 2021  5     47,889     37   11   52.8     369\n",
      "2021-06JUN.csv                 2021  6     47,749     37   11   52.8     210\n",
      "2021-07JUL.csv                 2021  7    748,713     48    0   49.0   11400\n",
      "2021-08AUG.CSV                 2021  8     47,451     37   11   53.1     553\n",
      "2021-09SEP.CSV                 2021  9     47,771     37   11   53.2     156\n",
      "2021-10OCT.CSV                 2021 10    187,848     48    0   48.7    1229\n",
      "2021-11NOV.CSV                 2021 11     47,417     37   11   52.9     109\n",
      "2021-12DEC.CSV                 2021 12     47,201     37   11   52.9     321\n",
      "2022-01JAN.csv                 2022  1    736,746     48    0   48.8    9660\n",
      "2022-02FEB.csv                 2022  2     45,889     37   11   52.7     262\n",
      "2022-03MAR.csv                 2022  3     46,154     37   11   52.8     214\n",
      "2022-04APR.csv                 2022  4    184,237     48    0   48.5    1082\n",
      "2022-05MAY.csv                 2022  5     46,264     39    9   51.8     307\n",
      "2022-06JUN.csv                 2022  6     45,894     39    9   51.7     270\n",
      "2022-07JUL.CSV                 2022  7    183,856     48    0   48.0    1376\n",
      "2022-08AUG.CSV                 2022  8     45,054     39    9   51.5     266\n",
      "2022-09SEP.CSV                 2022  9     46,261     39    9   51.6     286\n",
      "2022-10OCT.CSV                 2022 10    183,602     48    0   48.2    1020\n",
      "2022-11NOV.CSV                 2022 11     45,561     39    9   51.2     257\n",
      "2022-12DEC.CSV                 2022 12     45,687     39    9   51.5     144\n",
      "2023-01JAN.CSV                 2023  1    184,113     48    0   48.0     914\n",
      "2023-02FEB.CSV                 2023  2     47,044     39    9   51.4     282\n",
      "2023-03MAR.CSV                 2023  3     46,212     39    9   51.4     283\n",
      "2023-04APR.CSV                 2023  4    181,424     48    0   47.8     789\n",
      "2023-05MAY.CSV                 2023  5     45,505     39    9   51.4     160\n",
      "2023-06JUN.CSV                 2023  6     46,060     39    9   51.4     232\n",
      "2023-07JUL.CSV                 2023  7    718,567     48    0   48.8    9295\n",
      "2023-08AUG.CSV                 2023  8     44,999     39    9   51.9     360\n",
      "2023-09SEP.CSV                 2023  9     44,659     39    9   51.8     300\n",
      "2023-10OCT.CSV                 2023 10    179,173     48    0   48.1     970\n",
      "2023-11NOV.CSV                 2023 11     44,609     39    9   51.7     135\n",
      "2023-12DEC.CSV                 2023 12     44,141     39    9   51.7     160\n",
      "2024-01JAN.CSV                 2024  1    707,981     48    0   48.6    7198\n",
      "2024-02FEB.CSV                 2024  2     44,598     39    9   51.6     201\n",
      "2024-03MAR.CSV                 2024  3     44,063     39    9   51.6     187\n",
      "2024-04APR.CSV                 2024  4    175,511     48    0   48.0    1777\n",
      "2024-07JUL.CSV                 2024  7    173,259     48    0   47.9    1386\n",
      "\n",
      "======================================================================\n",
      "B. UNMAPPED RAW VARIABLES\n",
      "   (columns in raw CSVs not used by any COLUMN_PRIORITY mapping)\n",
      "======================================================================\n",
      "\n",
      "Total unmapped variables: 108\n",
      "Saved: unmapped_variables.csv\n",
      "\n",
      "variable                       files year_range  \n",
      "--------------------------------------------------\n",
      "PUFURB2015                        35 2020-2023   \n",
      "URB2K70                           33 2007-2016   \n",
      "J02_OTOC                          33 2007-2016   \n",
      "J04_OCLS                          33 2007-2016   \n",
      "J05_OHRS                          33 2007-2016   \n",
      "J06_OBIS                          33 2007-2016   \n",
      "A08_PQTR                          33 2007-2016   \n",
      "PUFC09A_WORK                      26 2021-2024   \n",
      "PUFC11A_PROVMUN                   26 2021-2024   \n",
      "PUFC09A_NFORMAL                   25 2018-2024   \n",
      "PUFC41_WQTR                       20 2016-2021   \n",
      "PUFURB2K10                        15 2016-2019   \n",
      "PUFC11A_ARRANGEMENT               14 2021-2024   \n",
      "PUFC12A_PROVMUN                   14 2021-2024   \n",
      "PROV                              12 2005-2018   \n",
      "A08_WPQTR                         11 2005-2012   \n",
      "PUFPRV                            10 2016-2018   \n",
      "PUFPRRCD                          10 2016-2018   \n",
      "SHSN                               8 2005-2006   \n",
      "HCN                                8 2005-2006   \n",
      "CTHHHU                             8 2005-2006   \n",
      "CRLNO                              8 2005-2006   \n",
      "CINTVW                             8 2005-2006   \n",
      "CAUXINFO                           8 2005-2006   \n",
      "CMBOUT                             8 2005-2006   \n",
      "CMBDIED                            8 2005-2006   \n",
      "CMBMARR                            8 2005-2006   \n",
      "CMBJOB                             8 2005-2006   \n",
      "CMBSTUD                            8 2005-2006   \n",
      "CMBOTHR                            8 2005-2006   \n",
      "  ... and 78 more (see unmapped_variables.csv)\n",
      "\n",
      "======================================================================\n",
      "C. MISSING TARGET COLUMNS MATRIX\n",
      "   (1 = missing from that file, 0 = present)\n",
      "======================================================================\n",
      "Saved: missing_columns_matrix.csv (105 files x 48 columns)\n",
      "\n",
      "target_column             missing_from    pct\n",
      "---------------------------------------------\n",
      "PUFHHNUM                         9/105   8.6%\n",
      "PUFHHSIZE                       41/105  39.0%\n",
      "PUFRPL                          38/105  36.2%\n",
      "PUFC08_CURSCH                   27/105  25.7%\n",
      "PUFC09_GRADTECH                 54/105  51.4%\n",
      "PUFC22_PFWRK                    10/105   9.5%\n",
      "PUFC24_PBASIS                   26/105  24.8%\n",
      "PUFC25_PBASIC                   26/105  24.8%\n",
      "PUFC27_NJOBS                    27/105  25.7%\n",
      "PUFC28_THOURS                    1/105   1.0%\n",
      "PUFC29_WWM48H                    1/105   1.0%\n",
      "PUFC31_FLWRK                    10/105   9.5%\n",
      "PUFC32_JOBSM                    26/105  24.8%\n",
      "PUFC33_WEEKS                    26/105  24.8%\n",
      "PUFC35_LTLOOKW                  27/105  25.7%\n",
      "PUFC37_WILLING                  27/105  25.7%\n",
      "PUFC39_YEAR                     65/105  61.9%\n",
      "PUFC39_MONTH                    65/105  61.9%\n",
      "PUFC43_QKB                       1/105   1.0%\n",
      "\n",
      "======================================================================\n",
      "D. TRANSLATION DROP REPORT\n",
      "   (values that were non-null in raw but became NaN after translation)\n",
      "======================================================================\n",
      "Saved: translation_drops.csv (80 records)\n",
      "\n",
      "target_column              total_drops  files_affected\n",
      "-------------------------------------------------------\n",
      "PUFC06_MSTAT                       113               8\n",
      "PUFC07_GRADE                    14,584              28\n",
      "PUFC11_WORK                    108,204               1\n",
      "PUFC12_JOB                      57,345              41\n",
      "PUFC31_FLWRK                         1               1\n",
      "PUFC38_PREVJOB                       1               1\n",
      "\n",
      "Dropped code values (top 10 per variable across all files):\n",
      "  PUFC06_MSTAT: 6(n=2,147)\n",
      "  PUFC07_GRADE: 900(n=158,479), 920(n=66,101), 910(n=31,977), 940(n=16,853), 930(n=3,686)\n",
      "  PUFC11_WORK: 7(n=1,886,586), 3(n=117,819), 5(n=20,710), 6(n=16,112), 4(n=14,649)\n",
      "  PUFC12_JOB: 3(n=1,071,219)\n",
      "  PUFC31_FLWRK: 0(n=19)\n",
      "  PUFC38_PREVJOB: 0(n=19)\n",
      "\n",
      "======================================================================\n",
      "E. CROSS-FILE CONSISTENCY CHECKS\n",
      "======================================================================\n",
      "  [FLAG] ROW_COUNT_OUTLIER: 2021-07JUL.csv has 748,713 rows (quarterly median=200,717, ratio=3.73)\n",
      "  [FLAG] ROW_COUNT_OUTLIER: 2022-01JAN.csv has 736,746 rows (quarterly median=200,717, ratio=3.67)\n",
      "  [FLAG] ROW_COUNT_OUTLIER: 2023-07JUL.CSV has 718,567 rows (quarterly median=200,717, ratio=3.58)\n",
      "  [FLAG] ROW_COUNT_OUTLIER: 2024-01JAN.CSV has 707,981 rows (quarterly median=200,717, ratio=3.53)\n",
      "\n",
      "Total flags: 4\n",
      "\n",
      "======================================================================\n",
      "F. AGGREGATE STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Total harmonized rows: 18,654,154\n",
      "Files processed: 105\n",
      "Files with errors: 0\n",
      "\n",
      "column                     avg_null%  min_null%  max_null%\n",
      "---------------------------------------------------------\n",
      "PUFREG                          0.00       0.00       0.00\n",
      "PUFSVYYR                        0.00       0.00       0.00\n",
      "PUFSVYMO                        0.00       0.00       0.00\n",
      "PUFHHNUM                        8.57       0.00     100.00\n",
      "PUFPSU                          0.00       0.00       0.00\n",
      "PUFHHSIZE                      39.05       0.00     100.00\n",
      "PUFRPL                         36.19       0.00     100.00\n",
      "PUFPWGTPRV                      0.00       0.00       0.00\n",
      "PUFC01_LNO                      1.12       0.00      59.14\n",
      "PUFC03_REL                      0.00       0.00       0.00\n",
      "PUFC04_SEX                      0.00       0.00       0.00\n",
      "PUFC05_AGE                      0.00       0.00       0.00\n",
      "PUFC06_MSTAT                    7.93       0.00      11.06\n",
      "PUFC07_GRADE                    8.33       6.36      11.06\n",
      "PUFC08_CURSCH                  68.46      49.85     100.00\n",
      "PUFC09_GRADTECH                65.88      26.20     100.00\n",
      "PUFC10_CONWR                   29.77       0.00      35.91\n",
      "PUFC11_WORK                    10.51       7.89      66.11\n",
      "PUFC12_JOB                     50.97      31.18      55.04\n",
      "PUFNEWEMPSTAT                  31.59      12.19      37.29\n",
      "PUFC14_PROCC                   58.72      53.20      67.47\n",
      "PUFC16_PKB                     58.72      53.20      67.47\n",
      "PUFC17_NATEM                   59.37      53.80      67.80\n",
      "PUFC18_PNWHRS                  59.37      53.80      67.80\n",
      "PUFC19_PHOURS                  59.34      53.80      67.80\n",
      "PUFC20_PWMORE                  59.37      53.80      67.80\n",
      "PUFC21_PLADDW                  59.68      53.80      93.66\n",
      "PUFC22_PFWRK                   63.34      53.80     100.00\n",
      "PUFC23_PCLASS                  59.34      53.80      67.80\n",
      "PUFC24_PBASIS                  83.66      73.86     100.00\n",
      "PUFC25_PBASIC                  85.89      77.21     100.00\n",
      "PUFC26_OJOB                    59.34      53.80      67.80\n",
      "PUFC27_NJOBS                   97.33      95.30     100.00\n",
      "PUFC28_THOURS                  59.71      53.80     100.00\n",
      "PUFC29_WWM48H                  84.18      72.72     100.00\n",
      "PUFC30_LOOKW                   72.22      50.82      75.16\n",
      "PUFC31_FLWRK                   98.88      98.36     100.00\n",
      "PUFC32_JOBSM                   99.07      98.36     100.00\n",
      "PUFC33_WEEKS                   99.07      98.36     100.00\n",
      "PUFC34_WYNOT                   73.50      52.12      76.71\n",
      "PUFC35_LTLOOKW                 99.28      98.57     100.00\n",
      "PUFC36_AVAIL                   95.72      50.82      97.77\n",
      "PUFC37_WILLING                 97.14      88.98     100.00\n",
      "PUFC38_PREVJOB                 72.22      51.10      75.16\n",
      "PUFC39_YEAR                    94.78      83.83     100.00\n",
      "PUFC39_MONTH                   94.78      83.83     100.00\n",
      "PUFC41_POCC                    85.31      77.84      88.11\n",
      "PUFC43_QKB                     70.57      57.39     100.00\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "COLUMN MAPPING AUDIT TRAIL (which raw column was used per target)\n",
      "----------------------------------------------------------------------\n",
      "  PUFREG                    -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFSVYYR                  -> SVYYR (2005-2015) | PUFSVYYR/SVYYR (2016) | PUFSVYYR (2017-2024)\n",
      "  PUFSVYMO                  -> SVYMO (2005-2015) | PUFSVYMO/SVYMO (2016) | PUFSVYMO (2017-2024)\n",
      "  PUFHHNUM                  -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFPSU                    -> PSU (2005-2015) | PSU/PUFPSU (2016) | PUFPSU (2017-2024)\n",
      "  PUFHHSIZE                 -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFRPL                    -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFPWGTPRV                -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC01_LNO                -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC03_REL                -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC04_SEX                -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC05_AGE                -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC06_MSTAT              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC07_GRADE              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC08_CURSCH             -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC09_GRADTECH           -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC10_CONWR              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC11_WORK               -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC12_JOB                -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFNEWEMPSTAT             -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC14_PROCC              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC16_PKB                -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC17_NATEM              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC18_PNWHRS             -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC19_PHOURS             -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC20_PWMORE             -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC21_PLADDW             -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC22_PFWRK              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC23_PCLASS             -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC24_PBASIS             -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC25_PBASIC             -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC26_OJOB               -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC27_NJOBS              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC28_THOURS             -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC29_WWM48H             -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC30_LOOKW              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC31_FLWRK              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC32_JOBSM              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC33_WEEKS              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC34_WYNOT              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC35_LTLOOKW            -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC36_AVAIL              -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC37_WILLING            -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC38_PREVJOB            -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC39_YEAR               -> MISSING (2005-2020) | MISSING/PUFC29_YEAR/PUFC39_YEAR (2021) | PUFC29_YEAR/PUFC39_YEAR (2022-2024)\n",
      "  PUFC39_MONTH              -> MISSING (2005-2020) | MISSING/PUFC29_MONTH/PUFC39_MONTH (2021) | PUFC29_MONTH/PUFC39_MONTH (2022-2024)\n",
      "  PUFC41_POCC               -> (varies, see per_file_diagnostics.csv)\n",
      "  PUFC43_QKB                -> (varies, see per_file_diagnostics.csv)\n"
     ]
    }
   ],
   "source": [
    "def generate_validation_report(diagnostics, output_path, output_schema):\n",
    "    \"\"\"Generate comprehensive validation reports from per-file diagnostics.\"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    report_lines = []\n",
    "\n",
    "    def rpt(line=\"\"):\n",
    "        report_lines.append(line)\n",
    "\n",
    "    # Filter to successful files only (those with 'raw_rows' key)\n",
    "    valid_diags = [d for d in diagnostics if \"raw_rows\" in d]\n",
    "    error_diags = [d for d in diagnostics if \"error\" in d]\n",
    "\n",
    "    rpt(\"=\" * 70)\n",
    "    rpt(\"LFS HARMONIZATION - VALIDATION REPORT\")\n",
    "    rpt(f\"Generated: {datetime.now().isoformat()}\")\n",
    "    rpt(f\"Files analyzed: {len(valid_diags)} successful, {len(error_diags)} errors\")\n",
    "    rpt(\"=\" * 70)\n",
    "\n",
    "    # ====================================================================\n",
    "    # A. PER-FILE SUMMARY TABLE\n",
    "    # ====================================================================\n",
    "    rpt(\"\\n\" + \"=\" * 70)\n",
    "    rpt(\"A. PER-FILE SUMMARY\")\n",
    "    rpt(\"=\" * 70)\n",
    "\n",
    "    summary_rows = []\n",
    "    for d in valid_diags:\n",
    "        overall_null = np.mean(list(d[\"null_rates\"].values())) if d[\"null_rates\"] else 0\n",
    "        total_trans_drops = sum(d[\"translation_drops\"].values()) if d[\"translation_drops\"] else 0\n",
    "        wt_zeros = d.get(\"weight_stats\", {}).get(\"zero_count\", 0)\n",
    "        wt_sum = d.get(\"weight_stats\", {}).get(\"sum\", 0)\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"file\": d[\"file_name\"],\n",
    "            \"year\": d[\"year\"],\n",
    "            \"month\": d[\"month\"],\n",
    "            \"raw_rows\": d[\"raw_rows\"],\n",
    "            \"raw_cols\": d[\"raw_cols\"],\n",
    "            \"harmonized_rows\": d[\"harmonized_rows\"],\n",
    "            \"mapped_cols\": d[\"mapped_count\"],\n",
    "            \"missing_cols\": d[\"missing_count\"],\n",
    "            \"avg_null_pct\": round(overall_null, 2),\n",
    "            \"weight_zeros\": wt_zeros,\n",
    "            \"weight_sum\": round(wt_sum, 0),\n",
    "            \"translation_drops\": total_trans_drops,\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    summary_df.to_csv(output_path / \"per_file_diagnostics.csv\", index=False)\n",
    "\n",
    "    rpt(f\"\\nSaved: per_file_diagnostics.csv ({len(summary_df)} rows)\")\n",
    "    rpt(f\"\\n{'file':<30s} {'year':>4s} {'mo':>2s} {'raw_rows':>10s} {'mapped':>6s} \"\n",
    "        f\"{'miss':>4s} {'null%':>6s} {'t_drops':>7s}\")\n",
    "    rpt(\"-\" * 75)\n",
    "    for _, r in summary_df.iterrows():\n",
    "        mo_str = str(int(r['month'])) if pd.notna(r['month']) else ''\n",
    "        rpt(f\"{r['file']:<30s} {int(r['year']):>4d} {mo_str:>2s} \"\n",
    "            f\"{int(r['raw_rows']):>10,d} {int(r['mapped_cols']):>6d} \"\n",
    "            f\"{int(r['missing_cols']):>4d} {r['avg_null_pct']:>6.1f} \"\n",
    "            f\"{int(r['translation_drops']):>7d}\")\n",
    "\n",
    "    # ====================================================================\n",
    "    # B. UNMAPPED VARIABLES REPORT\n",
    "    # ====================================================================\n",
    "    rpt(\"\\n\" + \"=\" * 70)\n",
    "    rpt(\"B. UNMAPPED RAW VARIABLES\")\n",
    "    rpt(\"   (columns in raw CSVs not used by any COLUMN_PRIORITY mapping)\")\n",
    "    rpt(\"=\" * 70)\n",
    "\n",
    "    unmapped_counter = Counter()\n",
    "    unmapped_by_year = {}\n",
    "    for d in valid_diags:\n",
    "        for col in d.get(\"unmapped_raw_columns\", []):\n",
    "            col_upper = col.upper()\n",
    "            unmapped_counter[col_upper] += 1\n",
    "            if col_upper not in unmapped_by_year:\n",
    "                unmapped_by_year[col_upper] = set()\n",
    "            unmapped_by_year[col_upper].add(d[\"year\"])\n",
    "\n",
    "    unmapped_rows = []\n",
    "    for var, count in unmapped_counter.most_common():\n",
    "        years = sorted(unmapped_by_year.get(var, []))\n",
    "        year_range = f\"{years[0]}-{years[-1]}\" if years else \"\"\n",
    "        unmapped_rows.append({\n",
    "            \"variable\": var,\n",
    "            \"file_count\": count,\n",
    "            \"year_range\": year_range,\n",
    "            \"years\": \",\".join(str(y) for y in years),\n",
    "        })\n",
    "\n",
    "    unmapped_df = pd.DataFrame(unmapped_rows)\n",
    "    unmapped_df.to_csv(output_path / \"unmapped_variables.csv\", index=False)\n",
    "\n",
    "    rpt(f\"\\nTotal unmapped variables: {len(unmapped_df)}\")\n",
    "    rpt(f\"Saved: unmapped_variables.csv\")\n",
    "    rpt(f\"\\n{'variable':<30s} {'files':>5s} {'year_range':<12s}\")\n",
    "    rpt(\"-\" * 50)\n",
    "    for _, r in unmapped_df.head(30).iterrows():\n",
    "        rpt(f\"{r['variable']:<30s} {int(r['file_count']):>5d} {r['year_range']:<12s}\")\n",
    "    if len(unmapped_df) > 30:\n",
    "        rpt(f\"  ... and {len(unmapped_df) - 30} more (see unmapped_variables.csv)\")\n",
    "\n",
    "    # ====================================================================\n",
    "    # C. MISSING TARGET COLUMNS MATRIX\n",
    "    # ====================================================================\n",
    "    rpt(\"\\n\" + \"=\" * 70)\n",
    "    rpt(\"C. MISSING TARGET COLUMNS MATRIX\")\n",
    "    rpt(\"   (1 = missing from that file, 0 = present)\")\n",
    "    rpt(\"=\" * 70)\n",
    "\n",
    "    matrix_rows = []\n",
    "    for d in valid_diags:\n",
    "        row = {\"file\": d[\"file_name\"], \"year\": d[\"year\"], \"month\": d[\"month\"]}\n",
    "        missing_set = set(d.get(\"missing_target_columns\", []))\n",
    "        for col in output_schema:\n",
    "            row[col] = 1 if col in missing_set else 0\n",
    "        matrix_rows.append(row)\n",
    "\n",
    "    matrix_df = pd.DataFrame(matrix_rows)\n",
    "    matrix_df.to_csv(output_path / \"missing_columns_matrix.csv\", index=False)\n",
    "    rpt(f\"Saved: missing_columns_matrix.csv ({len(matrix_df)} files x {len(output_schema)} columns)\")\n",
    "\n",
    "    # Summarize: which columns are most frequently missing\n",
    "    miss_totals = {}\n",
    "    for col in output_schema:\n",
    "        miss_totals[col] = int(matrix_df[col].sum())\n",
    "\n",
    "    rpt(f\"\\n{'target_column':<25s} {'missing_from':>12s} {'pct':>6s}\")\n",
    "    rpt(\"-\" * 45)\n",
    "    for col in output_schema:\n",
    "        cnt = miss_totals[col]\n",
    "        if cnt > 0:\n",
    "            pct = round(cnt / len(matrix_df) * 100, 1)\n",
    "            rpt(f\"{col:<25s} {cnt:>8d}/{len(matrix_df):<3d} {pct:>5.1f}%\")\n",
    "\n",
    "    # ====================================================================\n",
    "    # D. TRANSLATION DROP REPORT\n",
    "    # ====================================================================\n",
    "    rpt(\"\\n\" + \"=\" * 70)\n",
    "    rpt(\"D. TRANSLATION DROP REPORT\")\n",
    "    rpt(\"   (values that were non-null in raw but became NaN after translation)\")\n",
    "    rpt(\"=\" * 70)\n",
    "\n",
    "    trans_rows = []\n",
    "    all_dropped_codes = {}  # target -> Counter of dropped codes across all files\n",
    "\n",
    "    for d in valid_diags:\n",
    "        for target, drops in d.get(\"translation_drops\", {}).items():\n",
    "            if drops > 0:\n",
    "                trans_rows.append({\n",
    "                    \"file\": d[\"file_name\"],\n",
    "                    \"year\": d[\"year\"],\n",
    "                    \"month\": d[\"month\"],\n",
    "                    \"target_column\": target,\n",
    "                    \"values_dropped\": drops,\n",
    "                    \"raw_rows\": d[\"raw_rows\"],\n",
    "                    \"drop_pct\": round(drops / d[\"raw_rows\"] * 100, 2) if d[\"raw_rows\"] > 0 else 0,\n",
    "                })\n",
    "            # Aggregate dropped code values\n",
    "            for target_col, code_dict in d.get(\"dropped_code_values\", {}).items():\n",
    "                if target_col not in all_dropped_codes:\n",
    "                    all_dropped_codes[target_col] = Counter()\n",
    "                for code_val, count in code_dict.items():\n",
    "                    all_dropped_codes[target_col][code_val] += count\n",
    "\n",
    "    trans_df = pd.DataFrame(trans_rows)\n",
    "    if len(trans_df) > 0:\n",
    "        trans_df = trans_df.sort_values([\"target_column\", \"year\", \"month\"])\n",
    "    trans_df.to_csv(output_path / \"translation_drops.csv\", index=False)\n",
    "    rpt(f\"Saved: translation_drops.csv ({len(trans_df)} records)\")\n",
    "\n",
    "    if len(trans_df) > 0:\n",
    "        # Summary by target column\n",
    "        rpt(f\"\\n{'target_column':<25s} {'total_drops':>12s} {'files_affected':>15s}\")\n",
    "        rpt(\"-\" * 55)\n",
    "        for target in sorted(trans_df[\"target_column\"].unique()):\n",
    "            subset = trans_df[trans_df[\"target_column\"] == target]\n",
    "            rpt(f\"{target:<25s} {int(subset['values_dropped'].sum()):>12,d} {len(subset):>15d}\")\n",
    "\n",
    "        # Show the actual dropped code values\n",
    "        rpt(\"\\nDropped code values (top 10 per variable across all files):\")\n",
    "        for target in sorted(all_dropped_codes.keys()):\n",
    "            codes = all_dropped_codes[target].most_common(10)\n",
    "            code_str = \", \".join(f\"{c}(n={n:,})\" for c, n in codes)\n",
    "            rpt(f\"  {target}: {code_str}\")\n",
    "    else:\n",
    "        rpt(\"\\nNo translation drops detected.\")\n",
    "\n",
    "    # ====================================================================\n",
    "    # E. CROSS-FILE CONSISTENCY CHECKS\n",
    "    # ====================================================================\n",
    "    rpt(\"\\n\" + \"=\" * 70)\n",
    "    rpt(\"E. CROSS-FILE CONSISTENCY CHECKS\")\n",
    "    rpt(\"=\" * 70)\n",
    "\n",
    "    flags = []\n",
    "\n",
    "    # E1. Row count outliers (separate quarterly vs monthly)\n",
    "    if len(summary_df) > 0:\n",
    "        # Determine if file is quarterly (Jan/Apr/Jul/Oct) or monthly\n",
    "        summary_df[\"freq\"] = summary_df[\"month\"].apply(\n",
    "            lambda m: \"quarterly\" if m in [1, 4, 7, 10] else \"monthly\"\n",
    "        )\n",
    "        for freq_type in [\"quarterly\", \"monthly\"]:\n",
    "            subset = summary_df[summary_df[\"freq\"] == freq_type]\n",
    "            if len(subset) > 2:\n",
    "                median_rows = subset[\"raw_rows\"].median()\n",
    "                for _, r in subset.iterrows():\n",
    "                    ratio = r[\"raw_rows\"] / median_rows if median_rows > 0 else 0\n",
    "                    if ratio < 0.5 or ratio > 1.5:\n",
    "                        flag = (f\"ROW_COUNT_OUTLIER: {r['file']} has {int(r['raw_rows']):,} rows \"\n",
    "                                f\"({freq_type} median={int(median_rows):,}, ratio={ratio:.2f})\")\n",
    "                        flags.append(flag)\n",
    "                        rpt(f\"  [FLAG] {flag}\")\n",
    "\n",
    "    # E2. Weight sum jumps (>30% change between adjacent periods)\n",
    "    if len(summary_df) > 1:\n",
    "        sorted_summary = summary_df.sort_values([\"year\", \"month\"]).reset_index(drop=True)\n",
    "        for i in range(1, len(sorted_summary)):\n",
    "            prev_wt = sorted_summary.iloc[i - 1][\"weight_sum\"]\n",
    "            curr_wt = sorted_summary.iloc[i][\"weight_sum\"]\n",
    "            if prev_wt > 0 and curr_wt > 0:\n",
    "                change = abs(curr_wt - prev_wt) / prev_wt\n",
    "                if change > 0.3:\n",
    "                    flag = (f\"WEIGHT_JUMP: {sorted_summary.iloc[i]['file']} weight sum changed \"\n",
    "                            f\"{change:.0%} from previous period\")\n",
    "                    flags.append(flag)\n",
    "                    rpt(f\"  [FLAG] {flag}\")\n",
    "\n",
    "    # E3. Sex ratio check\n",
    "    sex_ratios = []\n",
    "    for d in valid_diags:\n",
    "        sd = d.get(\"sex_distribution\", {})\n",
    "        male = sd.get(\"1\", 0)\n",
    "        female = sd.get(\"2\", 0)\n",
    "        if male + female > 0:\n",
    "            ratio = male / (male + female)\n",
    "            sex_ratios.append((d[\"file_name\"], ratio))\n",
    "\n",
    "    if sex_ratios:\n",
    "        median_sex_ratio = np.median([r for _, r in sex_ratios])\n",
    "        for fname, ratio in sex_ratios:\n",
    "            if abs(ratio - median_sex_ratio) > 0.10:\n",
    "                flag = (f\"SEX_RATIO: {fname} male ratio={ratio:.3f} \"\n",
    "                        f\"(median={median_sex_ratio:.3f})\")\n",
    "                flags.append(flag)\n",
    "                rpt(f\"  [FLAG] {flag}\")\n",
    "\n",
    "    # E4. Age implausible values\n",
    "    for d in valid_diags:\n",
    "        astats = d.get(\"age_stats\", {})\n",
    "        neg = astats.get(\"implausible_negative\", 0)\n",
    "        over120 = astats.get(\"implausible_over_120\", 0)\n",
    "        if neg > 0 or over120 > 0:\n",
    "            flag = f\"AGE_IMPLAUSIBLE: {d['file_name']} has {neg} negative, {over120} over-120 ages\"\n",
    "            flags.append(flag)\n",
    "            rpt(f\"  [FLAG] {flag}\")\n",
    "\n",
    "    if not flags:\n",
    "        rpt(\"  No consistency flags raised.\")\n",
    "\n",
    "    rpt(f\"\\nTotal flags: {len(flags)}\")\n",
    "\n",
    "    # ====================================================================\n",
    "    # F. AGGREGATE STATISTICS\n",
    "    # ====================================================================\n",
    "    rpt(\"\\n\" + \"=\" * 70)\n",
    "    rpt(\"F. AGGREGATE STATISTICS\")\n",
    "    rpt(\"=\" * 70)\n",
    "\n",
    "    total_rows = sum(d[\"harmonized_rows\"] for d in valid_diags)\n",
    "    rpt(f\"\\nTotal harmonized rows: {total_rows:,}\")\n",
    "    rpt(f\"Files processed: {len(valid_diags)}\")\n",
    "    rpt(f\"Files with errors: {len(error_diags)}\")\n",
    "\n",
    "    # Overall null rate per column\n",
    "    rpt(f\"\\n{'column':<25s} {'avg_null%':>10s} {'min_null%':>10s} {'max_null%':>10s}\")\n",
    "    rpt(\"-\" * 57)\n",
    "    for col in output_schema:\n",
    "        rates = [d[\"null_rates\"].get(col, 100.0) for d in valid_diags]\n",
    "        if rates:\n",
    "            rpt(f\"{col:<25s} {np.mean(rates):>10.2f} {min(rates):>10.2f} {max(rates):>10.2f}\")\n",
    "\n",
    "    # Column mapping audit trail: which source was used for each target, by year\n",
    "    rpt(\"\\n\" + \"-\" * 70)\n",
    "    rpt(\"COLUMN MAPPING AUDIT TRAIL (which raw column was used per target)\")\n",
    "    rpt(\"-\" * 70)\n",
    "    for target in output_schema:\n",
    "        sources_by_year = {}\n",
    "        for d in valid_diags:\n",
    "            src = d.get(\"source_column_used\", {}).get(target)\n",
    "            yr = d[\"year\"]\n",
    "            if yr not in sources_by_year:\n",
    "                sources_by_year[yr] = set()\n",
    "            sources_by_year[yr].add(src if src else \"MISSING\")\n",
    "\n",
    "        # Summarize: group consecutive years with same source\n",
    "        year_source_pairs = sorted(sources_by_year.items())\n",
    "        summary_parts = []\n",
    "        for yr, srcs in year_source_pairs:\n",
    "            src_str = \"/\".join(sorted(srcs))\n",
    "            summary_parts.append(f\"{yr}:{src_str}\")\n",
    "\n",
    "        # Compress if many years have same source\n",
    "        if len(set(str(s) for _, s in year_source_pairs)) <= 3:\n",
    "            # Group by source\n",
    "            source_to_years = {}\n",
    "            for yr, srcs in year_source_pairs:\n",
    "                key = \"/\".join(sorted(srcs))\n",
    "                if key not in source_to_years:\n",
    "                    source_to_years[key] = []\n",
    "                source_to_years[key].append(yr)\n",
    "\n",
    "            parts = []\n",
    "            for src, yrs in source_to_years.items():\n",
    "                if len(yrs) > 2:\n",
    "                    parts.append(f\"{src} ({min(yrs)}-{max(yrs)})\")\n",
    "                else:\n",
    "                    parts.append(f\"{src} ({','.join(str(y) for y in yrs)})\")\n",
    "            rpt(f\"  {target:<25s} -> {' | '.join(parts)}\")\n",
    "        else:\n",
    "            rpt(f\"  {target:<25s} -> (varies, see per_file_diagnostics.csv)\")\n",
    "\n",
    "    # ====================================================================\n",
    "    # SAVE OUTPUTS\n",
    "    # ====================================================================\n",
    "    # Human-readable report\n",
    "    report_path = output_path / \"validation_report.txt\"\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(report_lines))\n",
    "\n",
    "    # Machine-readable JSON report\n",
    "    json_report = {\n",
    "        \"generated\": datetime.now().isoformat(),\n",
    "        \"files_analyzed\": len(valid_diags),\n",
    "        \"files_with_errors\": len(error_diags),\n",
    "        \"total_harmonized_rows\": total_rows,\n",
    "        \"flags\": flags,\n",
    "        \"unmapped_variable_count\": len(unmapped_df),\n",
    "        \"missing_column_summary\": miss_totals,\n",
    "        \"translation_drop_summary\": {\n",
    "            target: int(trans_df[trans_df[\"target_column\"] == target][\"values_dropped\"].sum())\n",
    "            for target in trans_df[\"target_column\"].unique()\n",
    "        } if len(trans_df) > 0 else {},\n",
    "        \"dropped_code_values\": {k: dict(v.most_common(10)) for k, v in all_dropped_codes.items()},\n",
    "    }\n",
    "    with open(output_path / \"validation_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_report, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"Validation report saved to: {report_path}\")\n",
    "    print(f\"JSON report saved to: {output_path / 'validation_report.json'}\")\n",
    "    print(f\"Per-file diagnostics: {output_path / 'per_file_diagnostics.csv'}\")\n",
    "    print(f\"Unmapped variables: {output_path / 'unmapped_variables.csv'}\")\n",
    "    print(f\"Missing columns matrix: {output_path / 'missing_columns_matrix.csv'}\")\n",
    "    print(f\"Translation drops: {output_path / 'translation_drops.csv'}\")\n",
    "    print(f\"\\nTotal flags raised: {len(flags)}\")\n",
    "\n",
    "    return report_lines, flags\n",
    "\n",
    "\n",
    "# Run the validation report\n",
    "if all_diagnostics:\n",
    "    report_lines, flags = generate_validation_report(\n",
    "        all_diagnostics, output_dir, OUTPUT_SCHEMA\n",
    "    )\n",
    "    # Display the text report in the notebook\n",
    "    print(\"\\n\".join(report_lines))\n",
    "else:\n",
    "    print(\"No diagnostics available. Run the harmonization first (Section 6).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
